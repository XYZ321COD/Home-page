<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Single Image Super Resolution with various prediction networks | Michal Znalezniak</title>
<meta name="keywords" content="deep learning, super resolution, unet, SISR, SR">
<meta name="description" content="The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image&rsquo;s spatial resolution while sharpening and clarifying its visual features.
While the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I&rsquo;ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction.">
<meta name="author" content="Michał Znaleźniak">
<link rel="canonical" href="https://michalznalezniak.com/posts/post2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://michalznalezniak.com/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://michalznalezniak.com/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://michalznalezniak.com/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://michalznalezniak.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://michalznalezniak.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://michalznalezniak.com/posts/post2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
    referrerpolicy="no-referrer">

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: false},
        {left: "\\[", right: "\\]", display: false},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
      ],
    });
  });
</script>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: true}
          ]
      });
  });
</script>
<meta property="og:title" content="Single Image Super Resolution with various prediction networks" />
<meta property="og:description" content="The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image&rsquo;s spatial resolution while sharpening and clarifying its visual features.
While the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I&rsquo;ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://michalznalezniak.com/posts/post2/" /><meta property="og:image" content="https://michalznalezniak.com/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-24T00:00:00+00:00" /><meta property="og:site_name" content="michal znalezniak machine learning deep learning artificial intelligence research" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://michalznalezniak.com/images/papermod-cover.png"/>

<meta name="twitter:title" content="Single Image Super Resolution with various prediction networks"/>
<meta name="twitter:description" content="The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image&rsquo;s spatial resolution while sharpening and clarifying its visual features.
While the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I&rsquo;ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://michalznalezniak.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Single Image Super Resolution with various prediction networks",
      "item": "https://michalznalezniak.com/posts/post2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Single Image Super Resolution with various prediction networks",
  "name": "Single Image Super Resolution with various prediction networks",
  "description": "The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image\u0026rsquo;s spatial resolution while sharpening and clarifying its visual features.\nWhile the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I\u0026rsquo;ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction.",
  "keywords": [
    "deep learning", "super resolution", "unet", "SISR", "SR"
  ],
  "articleBody": "The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image’s spatial resolution while sharpening and clarifying its visual features.\nWhile the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I’ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction. But before diving into these methods, it’s important to broadly discuss the datasets, neural network architecture, metrics, training parameters and benchmarks used when comparing these approaches.\nDatasets Training For this state of the project, DF2K dataset (DIV2K [1] + Flicker2K [2]) is used. The dataset consists of 3450 images. The low-resolution images are generated from ground truth images with bicubic downsampling.\nState-of-the-art algorithms are evaluated on three standard upscaling scales: 2x, 3x, and 4x. For this post the model will be trained and evaluated only on a 2x upscaling scale.\nValidation The performance is measured and compared on three standard benchmark datasets Set5 [3], Set14 [4] and BSD100 [5]. Example images from datasets can be seen in Figure 1.\nFigure 1: Example ground truth images from the datasets: Set5 (left), BSD100 (middle), and SET14 (right).\nMetrics In line with SRCNN [6], VDSR [7], and other similar methods, PSNR and SSIM metrics were calculated on the luminance component of the image. To ensure consistency with SRCNN and VDSR, a 2-pixel border was excluded from all sides of the image before calculating metrics.\nNetwork Architecture At this stage of the project, the basic U-Net architecture [8] is employed to extract features. Extracted features are then fed into a Predictor Network ($PN$), which transforms them into an upscaled version of the image. The prediction network differs depending on whether it is predicting color, residual, or kernels. The U-Net network, which functions as a feature extractor ($FN$), is combined with the prediction network ($PN$) to construct the whole model denoted as $N$.\nSimilar to the SRCNN [6], VDSR [7], and DRCN [9] networks, the model $N$ aims to learn an end-to-end mapping function $F$ between the bicubic-interpolated low-resolution image $I^{SR}$ and the high-resolution image $I^{HR}$.\nIn the original U-Net architecture, the decoder phase uses transposed convolution for upsampling. However, the current network architecture differs from the original U-Net, as it utilizes bicubic upscaling instead of transposed convolution, see Figure 2. The basic building block for U-Net architecture can be seen on Figure 3.\nFigure 2: U-Net architecture used for this project. The numbers above the features (purple rectangles) correspond to the number of feature channels.\nFigure 3: The structure of U-Net block.\rPrediction Networks Color Prediction Introduction A pioneering method based on neural networks in the field of single image super-resolution that directly predicts color is SRCNN [6]. The SRCNN model architecture consists of two main components:\nFeature Extraction: Two convolutional layers. The first layer uses a kernel size of $9×9$, followed by ReLU activation, and the second layer uses a kernel size of $5×5$. Reconstruction/Prediction Layer: A convolutional layer with a kernel size of $5×5$ that outputs three channels (RGB). Setup As previously discussed in this stage of the project, the feature extraction ($FE$) component is implemented using the U-Net network. The prediction network ($PN$) outputs three channels with a single convolutional layer using a $1×1$ kernel size, followed by a sigmoid activation function, which constrains the output within a 0-1 range.\nFor color prediction, $N$ directly reconstructs a high-resolution image $I^{HR}$.\n$$ N(x) = PN(FE(x)) \\approx I^{HR}$$\nFigure 4: Example input, prediction and ground-truth patch from Set5 for color prediction.\rResidual Prediction Introduction Residual prediction was introduced and popularized by VDSR [7]. In this approach, the model predicts residuals $I^{R}$ (the difference between the high-resolution and low-resolution images) rather than the high-resolution image directly (color prediction).\nWhen the network is trained to predict the entire high-resolution image, as in SRCNN, it must learn both low-frequency $I^{SR}$ and high-frequency information $I^{R}$. Low-frequency information is relatively simple and repetitive but it can still slow down training if the model focuses on learning it [7].\nResidual learning shifts the network’s focus to learning the more complex high-frequency components that are not present in the low-resolution input. This approach simplifies the learning task and speeds up convergence during the training [7].\nSetup The prediction network ($PN$) outputs three channels with a single convolutional layer using a $1×1$ kernel size.\nFor residual prediction, $N$ predicts the missing high-frequency details of $I^{SR}$, known as a residual part of the image $I^{R}$. The residual part of the image is defined as $I^{R} = I^{HR} - I^{SR}$, see Figure 5:\n$$ N(x) = PN(FE(x)) + x \\approx I^{HR}$$\nFigure 5: Example input, prediction and ground-truth patch from Set5 for residual prediction.\rKernel Prediction Introduction The kernel prediction network (KPN) [10] was introduced to address image denoising, an image restoration problem similar to super-resolution. KPN employs a CNN to estimate local weighting kernels, which are used to compute each denoised pixel based on its neighbors.\nFor all image restoration problems, including super resolution the kernel prediction network ensures that the final color estimation always remains within the boundary formed by the neighboring pixels in the input image [10], see Figure 6.\nThis significantly reduces the output value search space compared to direct color prediction methods, helping to avoid potential hallucination artifacts.\nFigure 6: On the left, pixel p (highlighted in red) and its neighborhood. In the center, the 3x3 filter predicted for the highlighted pixel. On the right, the super-resolved p after applying filtering.\rSetup Kernel prediction uses a single-layer convolutional prediction network ($PN$) that outputs a kernel of scalar weights that is applied to the blurry/low-frequency neighborhood of pixel $p$ to produce super resolved $\\tilde p$, see Figure 6.\nLetting $N(p)$ be the $k \\times k$ neighborhood centered around pixel $p$ , the final layer for each input pixel outputs kernel $z_{p} \\in \\mathbb{R}^{k \\times k}$. The kernel size $k$ is specified before training along with the other network hyperparameters and the same weights are applied to each RGB color channel. Experiments were conducted with $k=3$\nLet’s define $\\left[\\mathbf{z}_{p}\\right]q$ as the q-th entry in the vector obtained by flattening $z{p}$, Using this, the final normalized kernel weights can be computed as:\n$$ w_{p q}=\\frac{\\exp \\left(\\left[\\mathbf{z}{p}\\right]q\\right)}{\\sum{q^{\\prime} \\in \\mathcal{N}(p)} \\exp \\left(\\left[\\mathbf{z}{p}\\right]_{q^{\\prime}}\\right)} $$\nand the super-resolved pixel $\\tilde p$ color as: $$ \\tilde p=\\sum_{q \\in \\mathcal{N}(p)} q w_{p q} $$\nExperiments Training Parameters Training is performed using RGB input patches of size $32×32$ extracted from the low-resolution image $I^{LR}$, paired with the corresponding high-resolution patches.\nThe model is trained with the Adam optimizer by setting $ \\beta_{1}=0.9 $, $ \\beta_{2}=0.9 $ and $ \\epsilon = 10^{−8}$.\nThe minibatch size is set to $16$. The learning rate is initialized as $10^{-3}$ and halved at every 10 epochs. All models are trained over $80$ epochs.\nFor all setups the networks minimize following loss function:\n$$ Loss = \\frac{1}{n} \\sum^{n}{i=1} \\lvert N(I^{SR}{i}) - I^{HR}_{i} \\rvert $$\nwhere $n$ is the number of samples in a mini-batch, $I^{HR}$ is ground-truth,and $I^{SR}$ is bicubic-upsampled low-resolution images.\nFigure 7: Learning curves on the training set.\rEvaluation of Different Prediction Networks Benchmark Results The quantitative evaluation results of the final models, calculated on public benchmark datasets, are shown in Table 1.\nDataset Bicubic Color Prediction Residual Prediction Kernel Prediction Set5 33.97 / 0.9358 33.89 / 0.9407 37.29 / 0.9596 36.66 / 0.9556 Set14 29.74 / 0.8549 30.00 / 0.8716 31.70 / 0.8966 31.24 / 0.8843 BSD100 30.32 / 0.8787 30.74 / 0.8904 32.89 / 0.9140 32.45 / 0.9045 Table 1: PSNR/SSIM for scale factor $\\times 2$ on datasets Set5, Set14, BSD100. Bold values indicate the best performance.\nThe network based on color prediction converges too slowly (see Figure 7) and underperforms compared to both residual prediction and kernel prediction (see Table 1). The slow convergence is due to the fact that the color prediction network does not carry low-frequency information throughout the network.\nThe residual prediction network yields the best results in terms of metrics, outperforming both color prediction and kernel prediction.\nThe qualitative results are presented in Figure 8. Models based on residual and kernel prediction successfully reconstruct the detailed textures and lines in the HR images and exhibit better-looking outputs compared to the model with color predction network which fails to reconstruct texture and lines.\nFigure 8: Super-resolution results of Image 20 (BSD100) with scale factor ×2.\rConclusions This post compares various prediction networks for single-image super-resolution (SISR). Models based on residual and kernel prediction converge faster and avoid blur artifacts present in upscaled images produced by the color prediction network. The next post will explore methods to improve the convergence speed of color prediction based model by modifying the feature extractor architecture.\nReferences [1] Lim, Bee, et al. “Enhanced deep residual networks for single image super-resolution.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.\n[2] Agustsson, Eirikur, and Radu Timofte. “Ntire 2017 challenge on single image super-resolution: Dataset and study.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.\n[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. 5\n[4] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pages 711–730. Springer, 2010\n[5] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416–423. IEEE, 2001..\n[6] Dong, Chao, et al. “Image super-resolution using deep convolutional networks.” IEEE transactions on pattern analysis and machine intelligence 38.2 (2015): 295-307.\n[7] Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. “Accurate image super-resolution using very deep convolutional networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[8] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015.\n[9] Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. “Deeply-recursive convolutional network for image super-resolution.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[10] Bako, Steve, et al. “Kernel-predicting convolutional networks for denoising Monte Carlo renderings.” ACM Trans. Graph. 36.4 (2017): 97-1. ",
  "wordCount" : "1740",
  "inLanguage": "en",
  "datePublished": "2024-03-24T00:00:00Z",
  "dateModified": "2024-03-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Michał Znaleźniak"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://michalznalezniak.com/posts/post2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Michal Znalezniak",
    "logo": {
      "@type": "ImageObject",
      "url": "https://michalznalezniak.com/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://michalznalezniak.com/" accesskey="h" title="Michal Znalezniak (Alt + H)">Michal Znalezniak</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li></li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://michalznalezniak.com/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://michalznalezniak.com/">Home</a>&nbsp;»&nbsp;<a href="https://michalznalezniak.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Single Image Super Resolution with various prediction networks
    </h1>
    <div class="post-meta"><span title='2024-03-24 00:00:00 +0000 UTC'>March 24, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Michał Znaleźniak

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#datasets" aria-label="Datasets">Datasets</a><ul>
                        
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#validation" aria-label="Validation">Validation</a></li></ul>
                </li>
                <li>
                    <a href="#metrics" aria-label="Metrics">Metrics</a></li>
                <li>
                    <a href="#network-architecture" aria-label="Network Architecture">Network Architecture</a></li>
                <li>
                    <a href="#prediction-networks" aria-label="Prediction Networks">Prediction Networks</a><ul>
                        
                <li>
                    <a href="#color-prediction" aria-label="Color Prediction">Color Prediction</a><ul>
                        
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#setup" aria-label="Setup">Setup</a></li></ul>
                </li>
                <li>
                    <a href="#residual-prediction" aria-label="Residual Prediction">Residual Prediction</a><ul>
                        
                <li>
                    <a href="#introduction-1" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#setup-1" aria-label="Setup">Setup</a></li></ul>
                </li>
                <li>
                    <a href="#kernel-prediction" aria-label="Kernel Prediction">Kernel Prediction</a><ul>
                        
                <li>
                    <a href="#introduction-2" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#setup-2" aria-label="Setup">Setup</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#experiments" aria-label="Experiments">Experiments</a><ul>
                        
                <li>
                    <a href="#training-parameters" aria-label="Training Parameters">Training Parameters</a></li>
                <li>
                    <a href="#evaluation-of-different-prediction-networks" aria-label="Evaluation of Different Prediction Networks">Evaluation of Different Prediction Networks</a><ul>
                        
                <li>
                    <a href="#benchmark-results" aria-label="Benchmark Results">Benchmark Results</a></li></ul>
                </li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The goal of single-image super-resolution (SISR) is to transform a low-resolution image into a high-resolution version, enhancing both its detail and quality. This process, called upscaling, increases the image&rsquo;s spatial resolution while sharpening and clarifying its visual features.</p>
<p>While the objective remains the same, the methods to achieve SISR can vary. In this post (Part 1), I&rsquo;ll explore three different neural network-based SISR approaches: color prediction, residual prediction, and kernel prediction. But before diving into these methods, it&rsquo;s important to broadly discuss the datasets, neural network architecture, metrics, training parameters and benchmarks used when comparing these approaches.</p>
<h1 id="datasets">Datasets<a hidden class="anchor" aria-hidden="true" href="#datasets">#</a></h1>
<h2 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h2>
<p>For this state of the project, DF2K dataset (DIV2K <a href="/posts/post2/#DIV2k">[1]</a> + Flicker2K <a href="/posts/post2/#Flicker2K">[2]</a>) is used. The dataset consists of 3450 images. The low-resolution images are generated from ground truth images with bicubic downsampling.</p>
<p>State-of-the-art algorithms are evaluated on three standard upscaling scales: 2x, 3x, and 4x. For this post the model will be trained and evaluated only on a 2x upscaling scale.</p>
<h2 id="validation">Validation<a hidden class="anchor" aria-hidden="true" href="#validation">#</a></h2>
<p>The performance is measured and compared on three standard benchmark datasets  Set5 <a href="/posts/post2/#SET5">[3]</a>, Set14 <a href="/posts/post2/#SET14">[4]</a> and  BSD100 <a href="/posts/post2/#BSD100">[5]</a>. Example images from datasets can be seen in Figure 1.</p>
<p><img loading="lazy" src="images/Figure_1.png" alt="Fig1"  />

<em>Figure 1: Example ground truth images from the datasets: Set5 (left), BSD100 (middle), and SET14 (right).</em></p>
<h1 id="metrics">Metrics<a hidden class="anchor" aria-hidden="true" href="#metrics">#</a></h1>
<p>In line with SRCNN <a href="/posts/post2/#SRCNN">[6]</a>, VDSR <a href="/posts/post2/#VDSR">[7]</a>, and other similar methods, PSNR and SSIM metrics were calculated on the luminance component of the image. To ensure consistency with SRCNN and VDSR, a 2-pixel border was excluded from all sides of the image before calculating metrics.</p>
<h1 id="network-architecture">Network Architecture<a hidden class="anchor" aria-hidden="true" href="#network-architecture">#</a></h1>
<p>At this stage of the project, the basic U-Net architecture <a href="/posts/post2/#U-Net">[8]</a> is employed to extract features. Extracted features are then fed into a Predictor Network ($PN$), which transforms them into an upscaled version of the image. The prediction network differs depending on whether it is predicting color, residual, or kernels. The U-Net network, which functions as a feature extractor ($FN$), is combined with the prediction network ($PN$) to construct the whole model denoted as $N$.</p>
<p>Similar to the SRCNN <a href="/posts/post2/#SRCC">[6]</a>, VDSR <a href="/posts/post2/#VDSR">[7]</a>, and  DRCN <a href="/posts/post2/#DRCN">[9]</a> networks, the model $N$ aims to learn an end-to-end mapping function $F$ between the bicubic-interpolated low-resolution image $I^{SR}$ and the high-resolution image $I^{HR}$.</p>
<p>In the original U-Net architecture, the decoder phase uses transposed convolution for upsampling. However, the current network architecture differs from the original U-Net, as it utilizes bicubic upscaling instead of transposed convolution, see Figure 2. The basic building block for U-Net architecture can be seen on Figure 3.</p>
<p><img loading="lazy" src="images/UNet-Arch-new.png" alt="Fig2"  />

<em>Figure 2: U-Net architecture used for this project. The numbers above the features (purple rectangles) correspond to the number of feature channels.</em></p>
<br>
<figure style="text-align: center;">
    <img width="33%" src="images/UNet-Block.png" alt="UNet Block">
    <figcaption>Figure 3: The structure of U-Net block.</figcaption>
</figure>
<h1 id="prediction-networks">Prediction Networks<a hidden class="anchor" aria-hidden="true" href="#prediction-networks">#</a></h1>
<h2 id="color-prediction">Color Prediction<a hidden class="anchor" aria-hidden="true" href="#color-prediction">#</a></h2>
<h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>A pioneering method based on neural networks in the field of single image super-resolution that directly predicts color is SRCNN <a href="/posts/post2/#SRCC">[6]</a>. The SRCNN model architecture consists of two main components:</p>
<ul>
<li><strong>Feature Extraction</strong>: Two convolutional layers. The first layer uses a kernel size of $9×9$, followed by ReLU activation, and the second layer uses a kernel size of $5×5$.</li>
<li><strong>Reconstruction/Prediction Layer</strong>: A convolutional layer with a kernel size of $5×5$ that outputs three channels (RGB).</li>
</ul>
<h3 id="setup">Setup<a hidden class="anchor" aria-hidden="true" href="#setup">#</a></h3>
<p>As previously discussed in this stage of the project, the feature extraction ($FE$) component is implemented using the U-Net network. The prediction network ($PN$) outputs three channels with a single convolutional layer using a $1×1$ kernel size, followed by a sigmoid activation function, which constrains the output within a 0-1 range.</p>
<p>For color prediction, $N$ directly reconstructs a high-resolution image $I^{HR}$.</p>
<p>$$ N(x) = PN(FE(x)) \approx I^{HR}$$</p>
<br>
<figure style="text-align: center;">
    <img  width="75%" src="images/Color_Prediction.png" alt="Residual Prediction">
    <figcaption>Figure 4: Example input, prediction and ground-truth patch from Set5 for color prediction.</figcaption>
</figure>
<h2 id="residual-prediction">Residual Prediction<a hidden class="anchor" aria-hidden="true" href="#residual-prediction">#</a></h2>
<h3 id="introduction-1">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction-1">#</a></h3>
<p>Residual prediction was introduced and popularized by VDSR <a href="/posts/post2/#VDSR">[7]</a>. In this approach, the model predicts residuals $I^{R}$ (the difference between the high-resolution and low-resolution images) rather than the high-resolution image directly (color prediction).</p>
<p>When the network is trained to predict the entire high-resolution image, as in SRCNN, it must learn both low-frequency $I^{SR}$ and high-frequency information $I^{R}$. Low-frequency information is relatively simple and repetitive but it can still slow down training if the model focuses on learning it <a href="/posts/post2/#VDSR">[7]</a>.</p>
<p>Residual learning shifts the network’s focus to learning the more complex high-frequency components that are not present in the low-resolution input. This approach simplifies the learning task and speeds up convergence during the training <a href="/posts/post2/#VDSR">[7]</a>.</p>
<h3 id="setup-1">Setup<a hidden class="anchor" aria-hidden="true" href="#setup-1">#</a></h3>
<p>The prediction network ($PN$) outputs three channels with a single convolutional layer using a $1×1$ kernel size.</p>
<p>For residual prediction, $N$ predicts the missing high-frequency details of $I^{SR}$, known as a residual part of the image $I^{R}$. The residual part of the image is defined as $I^{R} = I^{HR} - I^{SR}$, see Figure 5:</p>
<p>$$ N(x) = PN(FE(x)) + x \approx I^{HR}$$</p>
<br>
<figure style="text-align: center;">
    <img  src="images/Residual_Prediction.png" alt="Residual Prediction">
    <figcaption>Figure 5: Example input, prediction and ground-truth patch from Set5 for residual prediction.</figcaption>
</figure>
<h2 id="kernel-prediction">Kernel Prediction<a hidden class="anchor" aria-hidden="true" href="#kernel-prediction">#</a></h2>
<h3 id="introduction-2">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction-2">#</a></h3>
<p>The kernel prediction network (KPN) <a href="/posts/post2/#KPN">[10]</a> was introduced to address image denoising, an image restoration problem similar to super-resolution. KPN employs a CNN to estimate local weighting kernels, which are used to compute each denoised pixel based on its neighbors.</p>
<p>For all image restoration problems, including super resolution the kernel prediction network ensures that the final color estimation always remains within the boundary formed by the neighboring pixels in the input image <a href="/posts/post2/#KPN">[10]</a>, see Figure 6.</p>
<p>This significantly reduces the output value search space compared to direct color prediction methods, helping to avoid potential hallucination artifacts.</p>
<figure style="text-align: center;">
    <img  src="images/Kernel_Pred_one_pixel.png" alt="Residual Prediction">
    <figcaption>Figure 6: On the left, pixel <i>p</i> (highlighted in red) and its neighborhood. In the center, the <i>3x3</i> filter predicted for the highlighted pixel. On the right, the super-resolved <i>p</i> after applying filtering.</figcaption>
</figure>
<h3 id="setup-2">Setup<a hidden class="anchor" aria-hidden="true" href="#setup-2">#</a></h3>
<p>Kernel prediction uses a single-layer convolutional prediction network ($PN$) that outputs a kernel of scalar weights that is applied to the blurry/low-frequency neighborhood of pixel $p$ to produce super resolved $\tilde p$, see Figure 6.</p>
<p>Letting $N(p)$ be the $k \times k$ neighborhood centered around pixel $p$ , the final layer for each input pixel outputs kernel $z_{p} \in \mathbb{R}^{k \times k}$. The kernel size $k$ is specified before training along with the other network hyperparameters and the same weights are applied to each RGB color channel. Experiments were conducted with $k=3$</p>
<p>Let&rsquo;s define  $\left[\mathbf{z}_{p}\right]<em>q$ as the  q-th entry in the vector obtained by flattening $z</em>{p}$, Using this, the final normalized kernel weights can be computed as:</p>
<p>$$
w_{p q}=\frac{\exp \left(\left[\mathbf{z}<em>{p}\right]<em>q\right)}{\sum</em>{q^{\prime} \in \mathcal{N}(p)} \exp \left(\left[\mathbf{z}</em>{p}\right]_{q^{\prime}}\right)}
$$</p>
<p>and the super-resolved pixel $\tilde p$ color as:
$$
\tilde p=\sum_{q \in \mathcal{N}(p)} q w_{p q}
$$</p>
<h1 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h1>
<h2 id="training-parameters">Training Parameters<a hidden class="anchor" aria-hidden="true" href="#training-parameters">#</a></h2>
<p>Training is performed using RGB input patches of size $32×32$ extracted from the low-resolution image $I^{LR}$, paired with the corresponding high-resolution patches.</p>
<p>The model is trained with the Adam optimizer by setting $ \beta_{1}=0.9 $,  $ \beta_{2}=0.9 $ and $ \epsilon = 10^{−8}$.</p>
<p>The minibatch size is set to $16$. The learning rate is initialized as $10^{-3}$ and halved at every 10 epochs.  All models are trained over $80$ epochs.</p>
<p>For all setups the networks minimize following loss function:</p>
<p>$$ Loss = \frac{1}{n} \sum^{n}<em>{i=1} \lvert N(I^{SR}</em>{i}) - I^{HR}_{i} \rvert $$</p>
<p>where $n$ is the number of samples in a mini-batch, $I^{HR}$ is ground-truth,and $I^{SR}$ is bicubic-upsampled low-resolution images.</p>
<figure style="text-align: center;">
    <img  src="images/Loss.png" alt="Loss">
    <figcaption>Figure 7: Learning curves on the training set.</figcaption>
</figure>
<h2 id="evaluation-of-different-prediction-networks">Evaluation of Different Prediction Networks<a hidden class="anchor" aria-hidden="true" href="#evaluation-of-different-prediction-networks">#</a></h2>
<h3 id="benchmark-results">Benchmark Results<a hidden class="anchor" aria-hidden="true" href="#benchmark-results">#</a></h3>
<p>The quantitative evaluation results of the final models, calculated on public benchmark datasets, are shown in Table 1.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Bicubic</th>
<th>Color Prediction</th>
<th>Residual Prediction</th>
<th>Kernel Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Set5</td>
<td>33.97 / 0.9358</td>
<td>33.89 / 0.9407</td>
<td><strong>37.29</strong> / <strong>0.9596</strong></td>
<td>36.66 / 0.9556</td>
</tr>
<tr>
<td>Set14</td>
<td>29.74 / 0.8549</td>
<td>30.00 / 0.8716</td>
<td><strong>31.70</strong>  / <strong>0.8966</strong></td>
<td>31.24 / 0.8843</td>
</tr>
<tr>
<td>BSD100</td>
<td>30.32 / 0.8787</td>
<td>30.74 / 0.8904</td>
<td><strong>32.89</strong>  / <strong>0.9140</strong></td>
<td>32.45 / 0.9045</td>
</tr>
</tbody>
</table>
<p>Table 1: PSNR/SSIM for scale factor $\times 2$ on datasets Set5, Set14, BSD100. <strong>Bold</strong> values indicate the best performance.</p>
<p>The network based on color prediction converges too slowly (see Figure 7) and underperforms compared to both residual prediction and kernel prediction (see Table 1). The slow convergence is due to the fact that the color prediction network does not carry low-frequency information throughout the network.</p>
<p>The residual prediction network yields the best results in terms of metrics, outperforming both color prediction and kernel prediction.</p>
<p>The qualitative results are presented in Figure 8. Models based on residual and kernel prediction successfully reconstruct the detailed textures and lines in the HR images and exhibit better-looking outputs compared to the model with color predction network which fails to reconstruct texture and lines.</p>
<br>
</br>
<figure style="text-align: center;">
    <img  src="images/Comaprison_for_all_predictors.png" alt="Comaprison_for_all_predictors">
    <figcaption>Figure 8: Super-resolution results of Image 20 (BSD100) with scale factor ×2.
    </figcaption>
</figure>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>This post compares various prediction networks for single-image super-resolution (SISR). Models based on residual and kernel prediction converge faster and avoid blur artifacts present in upscaled images produced by the color prediction network. The next post will explore methods to improve the convergence speed of color prediction based model by modifying the feature extractor architecture.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p><a id="DIV2k">[1] Lim, Bee, et al. &ldquo;Enhanced deep residual networks for single image super-resolution.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.</a></p>
<p><a id="Flicker2K">[2] Agustsson, Eirikur, and Radu Timofte. &ldquo;Ntire 2017 challenge on single image super-resolution: Dataset and study.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.</a></p>
<p><a id="SET5">[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. 5</a></p>
<p><a id="SET14">[4] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pages 711–730. Springer, 2010</a></p>
<p><a id="BSD100"> [5] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics. In Proceedings Eighth IEEE
International Conference on Computer Vision. ICCV 2001, volume 2, pages 416–423. IEEE, 2001..</a></p>
<p><a id="SRCNN">[6] Dong, Chao, et al. &ldquo;Image super-resolution using deep convolutional networks.&rdquo; IEEE transactions on pattern analysis and machine intelligence 38.2 (2015): 295-307.</a></p>
<p><a id="VDSRR">[7] Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. &ldquo;Accurate image super-resolution using very deep convolutional networks.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</a></p>
<p><a id="U-Net">[8] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. &ldquo;U-net: Convolutional networks for biomedical image segmentation.&rdquo; Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015.</a></p>
<p><a id="DRCN"> [9] Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. &ldquo;Deeply-recursive convolutional network for image super-resolution.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</a></p>
<p><a id="KPN"> [10] Bako, Steve, et al. &ldquo;Kernel-predicting convolutional networks for denoising Monte Carlo renderings.&rdquo; ACM Trans. Graph. 36.4 (2017): 97-1. </a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://michalznalezniak.com/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://michalznalezniak.com/tags/super-resolution/">Super Resolution</a></li>
      <li><a href="https://michalznalezniak.com/tags/unet/">Unet</a></li>
      <li><a href="https://michalznalezniak.com/tags/sisr/">SISR</a></li>
      <li><a href="https://michalznalezniak.com/tags/sr/">SR</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://michalznalezniak.com/posts/post1/">
    <span class="title">« Prev</span>
    <br>
    <span>Contrastive Hierarchical Clustering</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on x"
            href="https://x.com/intent/tweet/?text=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f&amp;hashtags=deeplearning%2csuperresolution%2cunet%2cSISR%2cSR">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f&amp;title=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks&amp;summary=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks&amp;source=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f&title=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks%20-%20https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on telegram"
            href="https://telegram.me/share/url?text=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Single Image Super Resolution with various prediction networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Single%20Image%20Super%20Resolution%20with%20various%20prediction%20networks&u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://michalznalezniak.com/">Michal Znalezniak</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
