<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Pytorch - How to speed up data loading | Michal Znalezniak</title>
<meta name="keywords" content="deep learning, super resolution, SISR, SR, pytorch, torch, profiling">
<meta name="description" content="In the Single Image Super Resolution with various prediction networks post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.
from glob import glob import torch from torch.utils.data import Dataset import torchvision from PIL import Image import numpy as np class SuperResolutionDataset(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self.">
<meta name="author" content="Michał Znaleźniak">
<link rel="canonical" href="https://michalznalezniak.com/posts/pytorch_how_to_speed_up_data_loading/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://michalznalezniak.com/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://michalznalezniak.com/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://michalznalezniak.com/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://michalznalezniak.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://michalznalezniak.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://michalznalezniak.com/posts/pytorch_how_to_speed_up_data_loading/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
    referrerpolicy="no-referrer">

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: false},
        {left: "\\[", right: "\\]", display: false},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
      ],
    });
  });
</script>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: true}
          ]
      });
  });
</script>
<meta property="og:title" content="Pytorch - How to speed up data loading" />
<meta property="og:description" content="In the Single Image Super Resolution with various prediction networks post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.
from glob import glob import torch from torch.utils.data import Dataset import torchvision from PIL import Image import numpy as np class SuperResolutionDataset(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://michalznalezniak.com/posts/pytorch_how_to_speed_up_data_loading/" /><meta property="og:image" content="https://michalznalezniak.com/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-19T00:00:00+00:00" /><meta property="og:site_name" content="michal znalezniak machine learning deep learning artificial intelligence research" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://michalznalezniak.com/images/papermod-cover.png"/>

<meta name="twitter:title" content="Pytorch - How to speed up data loading"/>
<meta name="twitter:description" content="In the Single Image Super Resolution with various prediction networks post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.
from glob import glob import torch from torch.utils.data import Dataset import torchvision from PIL import Image import numpy as np class SuperResolutionDataset(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://michalznalezniak.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Pytorch - How to speed up data loading",
      "item": "https://michalznalezniak.com/posts/pytorch_how_to_speed_up_data_loading/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch - How to speed up data loading",
  "name": "Pytorch - How to speed up data loading",
  "description": "In the Single Image Super Resolution with various prediction networks post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.\nfrom glob import glob import torch from torch.utils.data import Dataset import torchvision from PIL import Image import numpy as np class SuperResolutionDataset(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self.",
  "keywords": [
    "deep learning", "super resolution", "SISR", "SR", "pytorch", "torch", "profiling"
  ],
  "articleBody": "In the Single Image Super Resolution with various prediction networks post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.\nfrom glob import glob import torch from torch.utils.data import Dataset import torchvision from PIL import Image import numpy as np class SuperResolutionDataset(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self.scale_int = int(scale[1]) self.dataset_path_HR = self.dataset_path + \"/\" + self.dataset_name_HR self.dataset_path_LR = self.dataset_path + \"/\" + self.dataset_name_LR file_list_HR = glob(self.dataset_path_HR + \"/*\") file_list_LR = glob(self.dataset_path_LR + \"/*\") self.data = [] for hr_file, lr_file in zip(file_list_HR, file_list_LR): self.data.append((hr_file, lr_file)) def __len__(self): return len(self.data) def __getitem__(self, idx): hr_path, lr_path = self.data[idx] # Load the image hr_image = Image.open(hr_path).convert(\"RGB\") lr_image = Image.open(lr_path).convert(\"RGB\") hr_image = torchvision.transforms.ToTensor()(hr_image) lr_image = torchvision.transforms.ToTensor()(lr_image) # Setup the crop top righr corner, height and width for LR crop_size_lr = 48 top_x_lr, top_y_lr = torch.randint(0, (lr_image.shape[1]) - crop_size_lr, (1,)), torch.randint(0, (lr_image.shape[2]) - crop_size_lr, (1,)) # Setup the crop top righr corner, height and width for HR top_x_hr, top_y_hr = self.scale_int * top_x_lr, self.scale_int * top_y_lr crop_size_hr = crop_size_lr * self.scale_int # Crop the data lr_image = lr_image[:, top_x_lr : top_x_lr + crop_size_lr, top_y_lr : top_y_lr + crop_size_lr] hr_image = hr_image[:, top_x_hr : top_x_hr + crop_size_hr, top_y_hr : top_y_hr + crop_size_hr] return lr_image, hr_image class Flicker2K(SuperResolutionDataSet): pass class DIV2K(SuperResolutionDataSet): pass Profiling The __getitem__() method for above datasets loads the data and has three main steps:\nLoading full low resolution (LR) and high resolution (HR) images with PIL.Image.load(). Casting both image objects into pytorch’s tensors with torchvision.transforms.ToTensor(). Cropping the LR and HR tensors into 48x48 and 96x96 patches, respectively. Let’s use the PyTorch profiler to measure the time required to load the data and compare it to the time spent on the forward pass, backward pass and optimizer step.\nThe profiler will be run on dataloaders with a batch size of 16 and num_workers set to 0.\nFigure 1: Profiling step showing execution time for data loading, forward pass, backward pass and optimizer step.\rTraining Phase Time (ms) Percentage Data Loading 1,603.047 92.51% Forward Pass 24.200 1.30% Backward Pass 64.505 3.70% Optimizer Step 41.063 2.30% Total 1,732.815 100.00% Table 1: Execution times for each step in milliseconds and their relative percentages.\nPre-patching training data From Table 1, it is clear that data loading is a bottleneck, significantly impacting the training process. Let’s explore potential solutions to speed up data loading.\nSince the network is trained on 48x48 patches, we can pre-crop or patch the original images. By doing so, we avoid loading large 2K/4K images and instead work with smaller sections, which should significantly improve data loading efficiency and accelerate training. For this approach, we will pre-patch the images to 128x128 for LR and 256x256 for HR, see Figure 2.\nFigure 2: Original 2K/4K image (left) vs. 256x256 pre-patched image (right).\rLet’s rerun the profiler after implementing the process of pre-croping 2K / 4K images into 128x128 / 256x256 patches.\nFigure 3: Profiling step showing execution time for data loading (pre-cropped), forward pass, backward pass and optimizer step.\rTraining Phase Time (ms) Percentage Data Loading 74.928 36.60% Forward Pass 24.200 11.80% Backward Pass 64.505 31.50% Optimizer Step 41.063 20.00% Total 204.696 100.00% Table 2: Execution times for each step in milliseconds and their relative percentages after cropping data.\nMoving from .png to .npy This is significant improvement (see Table 2), but if we inspect closely we will see that right now torch.to_tensor takes a lot of time for the get_item method, see Figure 4.\nFigure 4: Profiling __getitem__() with .png images.\rLet’s save pre cropped/patched images to .npy format instead of .png, and let’s use numpy.load instead of PIL.load. Let’s rerun the profiler.\nTraining Phase Time (ms) Percentage Data Loading 31.912 19.73% Forward Pass 24.200 14.96% Backward Pass 64.505 39.89% Optimizer Step 41.063 25.39% Total 161.68 100.00% Table 3: Execution times for each step in milliseconds and their relative percentages, when PIL.load is replaced by numpy.load.\nNew dataset After the changes the code snippet for both datasets looks as following:\nclass SuperResolutionDataSet(Dataset): def __init__(self, dataset_path, dataset_name_HR, dataset_name_LR, scale, num_samples, load_npy, crop_size, name): self.name = name self.dataset_path = dataset_path self.dataset_name_HR = dataset_name_HR self.dataset_name_LR = dataset_name_LR self.scale = scale self.scale_int = int(scale[1]) self.load_np = load_npy self.crop_size_lr = None if crop_size == 'None' else crop_size dataset_path_LR = self.dataset_path + \"/\" + self.dataset_name_LR dataset_path_HR = self.dataset_path + \"/\" + self.dataset_name_HR self.data = SuperResolutionDataSet.load_files_to_array(dataset_path_LR, dataset_path_HR) self.num_samples = len(self.data) if num_samples == -1 else num_samples @staticmethod def load_files_to_array(dataset_path_LR, dataset_path_HR): file_list_LR = glob(dataset_path_LR + \"/*\") file_list_HR = glob(dataset_path_HR + \"/*\") data = [] for lr_file, hr_file in zip(file_list_LR, file_list_HR): data.append((lr_file, hr_file)) return data @staticmethod def _load_npys(lr_path, hr_path): lr_image, hr_image = np.load(lr_path) , np.load(hr_path) lr_image, hr_image = torch.from_numpy(lr_image).float(), torch.from_numpy(hr_image).float() return lr_image, hr_image @staticmethod def _load_pngs(lr_path, hr_path): lr_image, hr_image = Image.open(lr_path).convert(\"RGB\"), Image.open(hr_path).convert(\"RGB\") lr_image, hr_image = torchvision.transforms.ToTensor()(lr_image), torchvision.transforms.ToTensor()(hr_image) return lr_image, hr_image def _crop_data(self, lr_image, hr_image): if self.crop_size_lr: top_x_lr, top_y_lr = torch.randint(0, (lr_image.shape[1]) - self.crop_size_lr, (1,)), torch.randint(0, (lr_image.shape[2]) - self.crop_size_lr, (1,)) # Setup the crop top righr corner, height and width for HR top_x_hr, top_y_hr = self.scale_int * top_x_lr, self.scale_int * top_y_lr crop_size_hr = self.crop_size_lr * self.scale_int # Crop the data lr_image = lr_image[:, top_x_lr : top_x_lr + self.crop_size_lr, top_y_lr : top_y_lr + self.crop_size_lr] hr_image = hr_image[:, top_x_hr : top_x_hr + crop_size_hr, top_y_hr : top_y_hr + crop_size_hr] return lr_image, hr_image def __len__(self): return self.num_samples def __getitem__(self, idx): lr_path, hr_path = self.data[idx] lr_image, hr_image = SuperResolutionDataSet._load_npys(lr_path, hr_path) if self.load_np else SuperResolutionDataSet._load_pngs(lr_path, hr_path) lr_image, hr_image = self._crop_data(lr_image, hr_image) return lr_image, hr_image class Flicker2K(SuperResolutionDataSet): pass class DIV2K(SuperResolutionDataSet): pass Training By optimizing the data loading process, we reduced one training cycle from 1,732.815 ms to 161.68 ms. Finally, let’s regress the num_workers and see how much training speedup per epoch we will get, after the changes.\nFigure 5: Comparing epoch time in seconds for the new and the old version of dataset depending on num_workers and number of images used in epoch.\rIn Figure 5, we observe that when processing 3,418 images per epoch, the optimal value for num_workers is 1, resulting in a training time of 10 seconds per epoch. In comparison, under the previous (old) version of the dataset, processing the same number of images took 105 seconds per epoch.\nFor the new dataset, when the number of images per epoch is increased from 3,418 to 20,616, the optimal value for num_workers shifts to 2.\nConclusions By pre-patching training images and saving them in .npy format, we reduced the training time for one epoch by a factor of 10x, decreasing it from 105 seconds to 10 seconds. This highlights the importance of profiling and optimizing data loading pipelines, enabling faster and more scalable training processes.\n",
  "wordCount" : "1110",
  "inLanguage": "en",
  "datePublished": "2025-01-19T00:00:00Z",
  "dateModified": "2025-01-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Michał Znaleźniak"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://michalznalezniak.com/posts/pytorch_how_to_speed_up_data_loading/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Michal Znalezniak",
    "logo": {
      "@type": "ImageObject",
      "url": "https://michalznalezniak.com/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://michalznalezniak.com/" accesskey="h" title="Michal Znalezniak (Alt + H)">Michal Znalezniak</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li></li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://michalznalezniak.com/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://michalznalezniak.com/">Home</a>&nbsp;»&nbsp;<a href="https://michalznalezniak.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Pytorch - How to speed up data loading
    </h1>
    <div class="post-meta"><span title='2025-01-19 00:00:00 +0000 UTC'>January 19, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Michał Znaleźniak

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#profiling" aria-label="Profiling">Profiling</a><ul>
                        
                <li>
                    <a href="#pre-patching-training-data" aria-label="Pre-patching training data">Pre-patching training data</a></li>
                <li>
                    <a href="#moving-from-png-to-npy" aria-label="Moving from .png to .npy">Moving from .png to .npy</a></li>
                <li>
                    <a href="#new-dataset" aria-label="New dataset">New dataset</a></li></ul>
                </li>
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In the <a href="https://michalznalezniak.com/posts/post2/">Single Image Super Resolution with various prediction networks</a> post, we used DIV2K and Flicker2K dataset for training. Below is a python snippet for both datasets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SuperResolutionDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset_path</span><span class="p">,</span> <span class="n">dataset_name_HR</span><span class="p">,</span> <span class="n">dataset_name_LR</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">=</span> <span class="n">dataset_path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_HR</span> <span class="o">=</span> <span class="n">dataset_name_HR</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_LR</span> <span class="o">=</span> <span class="n">dataset_name_LR</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">scale</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path_HR</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&#34;/&#34;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_HR</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path_LR</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&#34;/&#34;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_LR</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">file_list_HR</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset_path_HR</span> <span class="o">+</span> <span class="s2">&#34;/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">file_list_LR</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset_path_LR</span> <span class="o">+</span> <span class="s2">&#34;/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">hr_file</span><span class="p">,</span> <span class="n">lr_file</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">file_list_HR</span><span class="p">,</span> <span class="n">file_list_LR</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">hr_file</span><span class="p">,</span> <span class="n">lr_file</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">hr_path</span><span class="p">,</span> <span class="n">lr_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Load the image</span>
</span></span><span class="line"><span class="cl">        <span class="n">hr_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">hr_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">lr_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hr_image</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">hr_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_image</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">lr_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Setup the crop top righr corner, height and width for LR</span>
</span></span><span class="line"><span class="cl">        <span class="n">crop_size_lr</span> <span class="o">=</span> <span class="mi">48</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_x_lr</span><span class="p">,</span> <span class="n">top_y_lr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">crop_size_lr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="n">crop_size_lr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Setup the crop top righr corner, height and width for HR</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_x_hr</span><span class="p">,</span> <span class="n">top_y_hr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">*</span> <span class="n">top_x_lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">*</span> <span class="n">top_y_lr</span>
</span></span><span class="line"><span class="cl">        <span class="n">crop_size_hr</span> <span class="o">=</span> <span class="n">crop_size_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Crop the data</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_image</span> <span class="o">=</span> <span class="n">lr_image</span><span class="p">[:,</span> <span class="n">top_x_lr</span> <span class="p">:</span> <span class="n">top_x_lr</span> <span class="o">+</span> <span class="n">crop_size_lr</span><span class="p">,</span> <span class="n">top_y_lr</span> <span class="p">:</span> <span class="n">top_y_lr</span> <span class="o">+</span> <span class="n">crop_size_lr</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">hr_image</span> <span class="o">=</span> <span class="n">hr_image</span><span class="p">[:,</span> <span class="n">top_x_hr</span> <span class="p">:</span> <span class="n">top_x_hr</span> <span class="o">+</span> <span class="n">crop_size_hr</span><span class="p">,</span> <span class="n">top_y_hr</span> <span class="p">:</span> <span class="n">top_y_hr</span> <span class="o">+</span> <span class="n">crop_size_hr</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Flicker2K</span><span class="p">(</span><span class="n">SuperResolutionDataSet</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DIV2K</span><span class="p">(</span><span class="n">SuperResolutionDataSet</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">pass</span>
</span></span></code></pre></div><h2 id="profiling">Profiling<a hidden class="anchor" aria-hidden="true" href="#profiling">#</a></h2>
<p>The <code>__getitem__()</code> method for above datasets loads the data and has three main steps:</p>
<ul>
<li>Loading full low resolution (LR) and high resolution (HR) images with <code>PIL.Image.load()</code>.</li>
<li>Casting both image objects into pytorch&rsquo;s tensors with <code>torchvision.transforms.ToTensor()</code>.</li>
<li>Cropping the LR and HR tensors into 48x48 and 96x96 patches, respectively.</li>
</ul>
<p>Let&rsquo;s use the PyTorch profiler to measure the time required to load the data and compare it to the time spent on the forward pass, backward pass and optimizer step.</p>
<p>The profiler will be run on dataloaders with a batch size of 16 and <code>num_workers</code> set to 0.</p>
<p>
<center><img src="images/Profler_old_dataset_full_profiler_step.png"></center>
<center><b>Figure 1</b>: Profiling step showing execution time for data loading, forward pass, backward pass and optimizer step.</center>
</p>
<div align="center">
<table>
<thead>
<tr>
<th>Training Phase</th>
<th>Time (ms)</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Loading</td>
<td>1,603.047</td>
<td>92.51%</td>
</tr>
<tr>
<td>Forward Pass</td>
<td>24.200</td>
<td>1.30%</td>
</tr>
<tr>
<td>Backward Pass</td>
<td>64.505</td>
<td>3.70%</td>
</tr>
<tr>
<td>Optimizer Step</td>
<td>41.063</td>
<td>2.30%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>1,732.815</strong></td>
<td><strong>100.00%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Table 1</strong>: Execution times for each step in milliseconds and their relative percentages.</p>
</div>
<br>
<h3 id="pre-patching-training-data">Pre-patching training data<a hidden class="anchor" aria-hidden="true" href="#pre-patching-training-data">#</a></h3>
<p>From Table 1, it is clear that data loading is a bottleneck, significantly impacting the training process. Let&rsquo;s explore potential solutions to speed up data loading.</p>
<p>Since the network is trained on 48x48 patches, we can pre-crop or patch the original images. By doing so, we avoid loading large 2K/4K images and instead work with smaller sections, which should significantly improve data loading efficiency and accelerate training. For this approach, we will pre-patch the images to 128x128 for LR and 256x256 for HR, see Figure 2.</p>
<p>
<center><img src="images/Patched_vs_Og.png"></center>
<center><b>Figure 2</b>: Original 2K/4K image (left) vs. 256x256 pre-patched image (right).</center>
</p>
<br>
<p>Let&rsquo;s rerun the profiler after implementing the process of pre-croping 2K / 4K images into 128x128 / 256x256 patches.</p>
<center><img src="images/Profler_new_dataset_full_profiler_step.png"></center>
<center><b>Figure 3</b>: Profiling step showing execution time for data loading (pre-cropped), forward pass, backward pass and optimizer step.</center>
<br>
<div align="center">
<table>
<thead>
<tr>
<th>Training Phase</th>
<th>Time (ms)</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Loading</td>
<td>74.928</td>
<td>36.60%</td>
</tr>
<tr>
<td>Forward Pass</td>
<td>24.200</td>
<td>11.80%</td>
</tr>
<tr>
<td>Backward Pass</td>
<td>64.505</td>
<td>31.50%</td>
</tr>
<tr>
<td>Optimizer Step</td>
<td>41.063</td>
<td>20.00%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>204.696</strong></td>
<td><strong>100.00%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Table 2</strong>: Execution times for each step in milliseconds and their relative percentages after cropping data.</p>
</div>
<br>
<h3 id="moving-from-png-to-npy">Moving from <code>.png</code> to <code>.npy</code><a hidden class="anchor" aria-hidden="true" href="#moving-from-png-to-npy">#</a></h3>
<p>This is significant improvement (see Table 2), but if we inspect closely we will see that right now <code>torch.to_tensor</code> takes a lot of time for the <code>get_item</code> method, see Figure 4.</p>
<center><img src="images/get_item_png.png"></center>
<center><b>Figure 4</b>: Profiling __getitem__() with .png images.</center>
<br>
<p>Let&rsquo;s save pre cropped/patched images to <code>.npy</code> format instead of <code>.png</code>, and let&rsquo;s use <code>numpy.load</code> instead of <code>PIL.load</code>. Let&rsquo;s rerun the profiler.</p>
<div align="center">
<table>
<thead>
<tr>
<th>Training Phase</th>
<th>Time (ms)</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Loading</td>
<td>31.912</td>
<td>19.73%</td>
</tr>
<tr>
<td>Forward Pass</td>
<td>24.200</td>
<td>14.96%</td>
</tr>
<tr>
<td>Backward Pass</td>
<td>64.505</td>
<td>39.89%</td>
</tr>
<tr>
<td>Optimizer Step</td>
<td>41.063</td>
<td>25.39%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>161.68</strong></td>
<td><strong>100.00%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Table 3</strong>: Execution times for each step in milliseconds and their relative percentages, when <code>PIL.load</code> is replaced by <code>numpy.load</code>.</p>
</div>
<h3 id="new-dataset">New dataset<a hidden class="anchor" aria-hidden="true" href="#new-dataset">#</a></h3>
<p>After the changes the code snippet for both datasets looks as following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SuperResolutionDataSet</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset_path</span><span class="p">,</span> <span class="n">dataset_name_HR</span><span class="p">,</span> <span class="n">dataset_name_LR</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">load_npy</span><span class="p">,</span> <span class="n">crop_size</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">=</span> <span class="n">dataset_path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_HR</span> <span class="o">=</span> <span class="n">dataset_name_HR</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_LR</span> <span class="o">=</span> <span class="n">dataset_name_LR</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">scale</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">load_np</span> <span class="o">=</span> <span class="n">load_npy</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">crop_size</span> <span class="o">==</span> <span class="s1">&#39;None&#39;</span> <span class="k">else</span> <span class="n">crop_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">dataset_path_LR</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&#34;/&#34;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_LR</span>
</span></span><span class="line"><span class="cl">        <span class="n">dataset_path_HR</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&#34;/&#34;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_name_HR</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">SuperResolutionDataSet</span><span class="o">.</span><span class="n">load_files_to_array</span><span class="p">(</span><span class="n">dataset_path_LR</span><span class="p">,</span> <span class="n">dataset_path_HR</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">num_samples</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">load_files_to_array</span><span class="p">(</span><span class="n">dataset_path_LR</span><span class="p">,</span> <span class="n">dataset_path_HR</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">file_list_LR</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="n">dataset_path_LR</span> <span class="o">+</span> <span class="s2">&#34;/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">file_list_HR</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="n">dataset_path_HR</span> <span class="o">+</span> <span class="s2">&#34;/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">lr_file</span><span class="p">,</span> <span class="n">hr_file</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">file_list_LR</span><span class="p">,</span> <span class="n">file_list_HR</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">lr_file</span><span class="p">,</span> <span class="n">hr_file</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_load_npys</span><span class="p">(</span><span class="n">lr_path</span><span class="p">,</span> <span class="n">hr_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">lr_path</span><span class="p">)</span> <span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">hr_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lr_image</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">hr_image</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_load_pngs</span><span class="p">(</span><span class="n">lr_path</span><span class="p">,</span> <span class="n">hr_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">lr_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">),</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">hr_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">lr_image</span><span class="p">),</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">hr_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_crop_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">top_x_lr</span><span class="p">,</span> <span class="n">top_y_lr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Setup the crop top righr corner, height and width for HR</span>
</span></span><span class="line"><span class="cl">            <span class="n">top_x_hr</span><span class="p">,</span> <span class="n">top_y_hr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">*</span> <span class="n">top_x_lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span> <span class="o">*</span> <span class="n">top_y_lr</span>
</span></span><span class="line"><span class="cl">            <span class="n">crop_size_hr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_int</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Crop the data</span>
</span></span><span class="line"><span class="cl">            <span class="n">lr_image</span> <span class="o">=</span> <span class="n">lr_image</span><span class="p">[:,</span> <span class="n">top_x_lr</span> <span class="p">:</span> <span class="n">top_x_lr</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span><span class="p">,</span> <span class="n">top_y_lr</span> <span class="p">:</span> <span class="n">top_y_lr</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size_lr</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">hr_image</span> <span class="o">=</span> <span class="n">hr_image</span><span class="p">[:,</span> <span class="n">top_x_hr</span> <span class="p">:</span> <span class="n">top_x_hr</span> <span class="o">+</span> <span class="n">crop_size_hr</span><span class="p">,</span> <span class="n">top_y_hr</span> <span class="p">:</span> <span class="n">top_y_hr</span> <span class="o">+</span> <span class="n">crop_size_hr</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_path</span><span class="p">,</span> <span class="n">hr_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="n">SuperResolutionDataSet</span><span class="o">.</span><span class="n">_load_npys</span><span class="p">(</span><span class="n">lr_path</span><span class="p">,</span> <span class="n">hr_path</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_np</span> <span class="k">else</span> <span class="n">SuperResolutionDataSet</span><span class="o">.</span><span class="n">_load_pngs</span><span class="p">(</span><span class="n">lr_path</span><span class="p">,</span> <span class="n">hr_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_crop_data</span><span class="p">(</span><span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">lr_image</span><span class="p">,</span> <span class="n">hr_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Flicker2K</span><span class="p">(</span><span class="n">SuperResolutionDataSet</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DIV2K</span><span class="p">(</span><span class="n">SuperResolutionDataSet</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">pass</span>
</span></span></code></pre></div><h2 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h2>
<p>By optimizing the data loading process, we reduced one training cycle from <code>1,732.815 ms</code> to <code>161.68 ms</code>. Finally, let&rsquo;s regress the <code>num_workers</code> and see how much training speedup per epoch we will get, after the changes.</p>
<center><img src="images/plot2.png"></center>
<center><b>Figure 5</b>: Comparing epoch time in seconds for the new and the old version of dataset depending on num_workers and number of images used in epoch.</center>
<br>
<p>In Figure 5, we observe that when processing <code>3,418</code> images per epoch, the optimal value for <code>num_workers</code> is 1, resulting in a training time of 10 seconds per epoch. In comparison, under the previous (old) version of the dataset, processing the same number of images took 105 seconds per epoch.</p>
<p>For the new dataset, when the number of images per epoch is increased from <code>3,418</code> to <code>20,616</code>, the optimal value for <code>num_workers</code> shifts to 2.</p>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>By pre-patching training images and saving them in <code>.npy</code> format, we reduced the training time for one epoch by a factor of <strong>10x</strong>, decreasing it from 105 seconds to 10 seconds. This highlights the importance of profiling and optimizing data loading pipelines, enabling faster and more scalable training processes.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://michalznalezniak.com/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://michalznalezniak.com/tags/super-resolution/">Super Resolution</a></li>
      <li><a href="https://michalznalezniak.com/tags/sisr/">SISR</a></li>
      <li><a href="https://michalznalezniak.com/tags/sr/">SR</a></li>
      <li><a href="https://michalznalezniak.com/tags/pytorch/">Pytorch</a></li>
      <li><a href="https://michalznalezniak.com/tags/torch/">Torch</a></li>
      <li><a href="https://michalznalezniak.com/tags/profiling/">Profiling</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://michalznalezniak.com/posts/post2/">
    <span class="title">Next »</span>
    <br>
    <span>Single Image Super Resolution with various prediction networks</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on x"
            href="https://x.com/intent/tweet/?text=Pytorch%20-%20How%20to%20speed%20up%20data%20loading&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f&amp;hashtags=deeplearning%2csuperresolution%2cSISR%2cSR%2cpytorch%2ctorch%2cprofiling">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f&amp;title=Pytorch%20-%20How%20to%20speed%20up%20data%20loading&amp;summary=Pytorch%20-%20How%20to%20speed%20up%20data%20loading&amp;source=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f&title=Pytorch%20-%20How%20to%20speed%20up%20data%20loading">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on whatsapp"
            href="https://api.whatsapp.com/send?text=Pytorch%20-%20How%20to%20speed%20up%20data%20loading%20-%20https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on telegram"
            href="https://telegram.me/share/url?text=Pytorch%20-%20How%20to%20speed%20up%20data%20loading&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Pytorch - How to speed up data loading on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Pytorch%20-%20How%20to%20speed%20up%20data%20loading&u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpytorch_how_to_speed_up_data_loading%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://michalznalezniak.com/">Michal Znalezniak</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
