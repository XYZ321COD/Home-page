<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Contrastive Hierarchical Clustering | Michal Znalezniak</title>
<meta name="keywords" content="deep learning, clustering, representation learning">
<meta name="description" content="In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distills the base network into a binary tree without access to any labeled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.
You can access the full paper of CoHiClust by clicking here.">
<meta name="author" content="Michał Znaleźniak">
<link rel="canonical" href="https://michalznalezniak.com/posts/post1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://michalznalezniak.com/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://michalznalezniak.com/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://michalznalezniak.com/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://michalznalezniak.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://michalznalezniak.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://michalznalezniak.com/posts/post1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
    referrerpolicy="no-referrer">

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: false},
        {left: "\\[", right: "\\]", display: false},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
      ],
    });
  });
</script>

<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: true}
          ]
      });
  });
</script>
<meta property="og:title" content="Contrastive Hierarchical Clustering" />
<meta property="og:description" content="In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distills the base network into a binary tree without access to any labeled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.
You can access the full paper of CoHiClust by clicking here." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://michalznalezniak.com/posts/post1/" /><meta property="og:image" content="https://michalznalezniak.com/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-24T00:00:00+00:00" /><meta property="og:site_name" content="michal znalezniak machine learning deep learning artificial intelligence research" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://michalznalezniak.com/images/papermod-cover.png"/>

<meta name="twitter:title" content="Contrastive Hierarchical Clustering"/>
<meta name="twitter:description" content="In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distills the base network into a binary tree without access to any labeled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.
You can access the full paper of CoHiClust by clicking here."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://michalznalezniak.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Contrastive Hierarchical Clustering",
      "item": "https://michalznalezniak.com/posts/post1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Contrastive Hierarchical Clustering",
  "name": "Contrastive Hierarchical Clustering",
  "description": "In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distills the base network into a binary tree without access to any labeled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.\nYou can access the full paper of CoHiClust by clicking here.",
  "keywords": [
    "deep learning", "clustering", "representation learning"
  ],
  "articleBody": "In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distills the base network into a binary tree without access to any labeled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.\nYou can access the full paper of CoHiClust by clicking here.\nFor those who are interested in exploring the code, data, and additional resources associated with this study, you can find them in my GitHub repository here.\nClustering Clustering, a fundamental branch of unsupervised learning, is often one of the first steps in data analysis, which finds applications in anomaly detection [1], document clustering [2], bioinformatins [3] and many more. Initial approaches use representations taken from pre-trained models [4, 5] or employ autoencoders in joint training of the representation and the flat clustering model [6] or hierarchical clustering model [7]. Recent models designed to image data frequently follow the self-supervised learning principle, where the representation is trained on pairs of similar images generated automatically by data augmentations [8, 9]. Since augmentations used for image data are class-invariant, the latter techniques of ten obtain a very high similarity to the ground truth classes. However, we should be careful when comparing clustering techniques only by inspecting their accuracy with ground truth classes because the objective of clustering is not to perform classification.\nObjective of Clustering So what’s the objective of clustering then? The primary objective of clustering is to discover structures and patterns in high-dimensional unlabeled data and group together data points with similar patterns. The above procedure reduces the complexity, facilitates the interpretation, and grants important insights into data. Let’s examine how much meaningful information a clustering algorithm can deliver when applied to a subset of the most widely recognized datasets in the field of computer vision and machine learning — ImageNet-10.\nFig. 1. Information delivered by hierarchical clustering CoHiClust for ImageNet-10. It is evident that images with soccer ball are similar to pictures with oranges because of their shapes. Dogs are more similar to leopards than to penguins, which is reflected in the constructed hierarchy. The same hold when analyzing the leafs representing cars, trucks and ships. Looking at the first hierarchy level, we observe a distinction on the right sub-tree representing machines and left-sub-tree dominated by animals. Moreover, balls and oranges are separated from the animal branch.\nThe algorithm reduces high-dimensional images into a hierarchy of groups that describe images and provides information that summarizes the dataset from high to low-level information. The hierarchy of groups is consistent with human intuition and image semantics, see Figure 1.\nFig. 2. Super-groups created by clustering algorithm for Image-Net10.\rThanks to the clustering we can understand that ImageNet-10 consists of two super-groups which can be categorazied as ‘machines’ and ’not machines’, four super-groups which can be catogorazied as ‘flying machines’, ’not flying machines’, ‘animals’ and ’not animals’, see Figure 2.\nHierarchical clustering Hierarchical clustering organizes data into a tree-like structure where clusters are nested within each other, while flat clustering forms non-overlapping clusters without any hierarchy.\nHierarchical clustering can provide more detailed information about the relationships between data points, as it captures the hierarchical structure of the data. This can be useful for exploring nested relationships and understanding similarities at different levels of granularity.\nHierarchical clustering groups data based on similarity, forming a hierarchical structure of clusters. Approaches to hierarchical clustering typically belong to two main groups.\nAgglomerative: This is a “bottom-up” approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive: This is a “top-down” approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. CoHiClust CoHiClust by backpropagation, jointly learns deep representation using SimCLR [10] framework and performs hierarchical clustering in a top-down manner. There are three key components of CoHiClust, see Figure 3.\nThe backbone neural network $f(.)$ that generates the representation used by the hierarchical clustering head. The hierarchical clustering head $\\pi(.)$, which assigns data points to clusters by a sequence of decisions. The regularized contrastive loss, which allows for training the whole framework. Fig. 3: Illustration of main components of CoHiClust.\rBackbone network CoHiClust utilizes ResNet architectures as its backbone network. This component projects images into an internal representation, which is then utilized by the clustering network. CoHiClust can rely on various architecture options. You can observe how change of backbone network influences the final quality of clustering in the full paper.\nHierarchical clustering head CoHiClust depends on a soft binary decision tree to create a hierarchical structure, where leaves play the role of clusters (similar to [11] ). In contrast to hard decision trees, every internal node defines the probability of taking a left/right branch. The final assignment of the input examples to clusters involves partial decisions made by the internal nodes. Aggregating these decisions induces the posterior probability over leaves\nEach inner node is model by one neuron from linear layer $\\pi(z) = [\\sigma (w_1^T z + b_1), \\ldots, \\sigma (w_K^T z + b_K)]$, where $w_n \\in \\R^N$ and $b_n \\in \\R$ are trainable parameters of $\\pi$ and $\\sigma$ is sigmoid function.\nFig. 4: Illustration of distilling neurnal network into soft decision tree. The output neurons of the projection head $\\pi$ (appended to the base network $g$) model decisions made by the internal tree nodes. The final assignment of the input example to the cluster (leaf node) is performed by aggregating edge probabilities located on the path from the root to this leaf\nWith $\\pi$ output we can define a probability distribution of assigning data to clusters on all levels of the tree, see Figure 4\nLoss functions CoHiClust is trained with hierarchical contrastive loss function designed for trees - CoHiLoss. CoHiClust builds hierarchical structure by maximizing the likelihood that similar data points will follow the same path. The more similar data points, the longer they should be routed through the same nodes. Clustering works in unsupervised setting, therefor CoHiClust uses a self-supervised approach and generates images using data augmentations (SimCLR).\nConsider two data points $x_1, x_2$ with posterior probabilities $P_t(x_1), P_t(x_2)$ at level $t$. The probability they reach the same node is $P_t(x_1) \\cdot P_t(x_2) = \\sum_{i=0}^{2^t-1} P_t^i(x_1) P_t^i(x_2)$, maximal if both are identical one-hot vectors. Let’s Define the similarity score $s_t(x_1,x_2) = \\sqrt{P_t(x_1) \\cdot P_t(x_2)} = \\sum_{i=0}^{2^t-1} \\sqrt{P_t^i(x_1) P_t^i(x_2)}$. Let’s aggregate it over all levels of the tree, the final similarity function is $s(x_1,x_2) = \\sum_{t=0}^{T-1} s_t(x_1,x_2)$.\nIn training, consider a minibatch $ [{x_j}]_{j=1}^N $ and its augmented view $ \\tilde{x_j} $. Each $(x_j,\\tilde{x}_j)$ pair is treated as positive, maximizing their similarity score to encourage them to share the same leaf node. Other pairs are treated as negative to avoid degenerate solutions, yielding the hierarchical contrastive loss:\n$$ CoHiLossNeg = \\frac{1}{N(N-1)} \\sum_{j=1}^N \\sum_{i\\neq j} s(x_j,\\tilde{x}_{i}) $$\n$$ CoHiLossPos = \\frac{1}{N} \\sum_{j=1}^N s(x_j,\\tilde{x}_{j}) $$\n$$ CoHiLoss = -CoHiLossNeg + CoHiLossPos $$\nMinimizing this loss maximizes the likelihood of similar data points sharing the same path (second term) and minimizes the likelihood of dissimilar ones being grouped together.\nResults CoHiClust was evaluated on several datasets of color images of various resolutions and with a diverse number of classes. In addition to reporting similarity scores with ground-truth partitions, the constructed hierarchies were analyzed, which is equally important in practical use-cases.\nTo measure the similarity of the constructed partition with the ground truth, four widely-used clustering metrics were applied: normalized mutual information (NMI), clustering accuracy (ACC), adjusted rand index (ARI), and dendrogram purity (DP).\nFig. 5: Tree hierarchy constructed for MNIST. Observe that neighboring leaves contain images of visually similar classes, e.g. 8 and 0; 4 and 9; 1 and 7. Such a property holds also for nodes in the upper levels of the tree – the left sub-tree contain digits with circular shapes, while the digits located in the right sub-tree consist of lines.\nComparison with deep hierarchical clustering methods In authors knowledge, DeepECT [7] is the only hierarchical clustering method based on deep neural networks. Following their experimental setup, we report the results on two popular image datasets, MNIST and F-MNIST, and consider classical hierarchical algorithms evaluated on the latent representation created by the autoencoder and IDEC [12]. The results summarized in Table 1 demonstrate that CoHiClust outperforms all baselines on both MNIST and F-MNIST datasets in terms of all metrics. Interestingly, DeepECT benefits from data augmentation in the case of MNIST, while on F-MNIST it deteriorates its performance. All methods except CoHiClust and DeepECT failed completely to create a hierarchy recovering true classes (see the DP measure), which confirms that there is a lack of powerful hierarchical clustering methods based on neural networks.\nTable 1: Comparison with hierarchical models in terms of DP, NMI and ACC (higher is better).\rWhile examining the MNIST and F-MNIST datasets, it’s apparent that CoHiClust produces clusters with a sensible structure. This structure is in line with our expectations and the inherent semantics of the images. Figure 5 and Figure 6 provide visual confirmation of this alignment.\nFig. 6: Tree hierarchy generated by CoHiClust for F-MNIST (images in the nodes denote mean images in each sub-tree). The right sub-tree contains clothes while the other items (shoes and bags) are placed in the left branch. Looking at the lowest hierarchy level, we have clothes with long sleeves grouped in the neighboring leaves. The same holds for clothes with designs. Observe that CoHiClust assigned white-colored t-shirts and dresses to the same cluster, while trousers are in the separate one. Small shoes such as sneakers or sandals are considered similar (neighboring leaves) and distinct from ankle shoes. Concluding, CoHiClust is able to retrieve meaningful information about image semantics, which is complementary to the ground truth classification.\nComparision with Agglomerative Hierarchical Clustering The top layer responsible for constructing a decision tree is an important component of CoHiClust and cannot be replaced by alternative hierarchical clustering methods. For this purpose, a backbone network is first trained with a typical self-supervised SimCLR technique. Next, agglomerative clustering is applied to the resulting representation. As seen in Table 2, agglomerative clustering yields very low results, indicating that joint optimization of the backbone network and clustering tree using the proposed CoHiLoss is a significantly better choice. Consequently, the representation taken from a typical self-supervised learning model does not provide a representation that can be clustered accurately using simple methods.\nTable 2: Comparison with agglomerative clustering trained on the representation\rgenerated by the self-supervised learning model on CIFAR10.\rComparision with deep flat clustering methods CoHiClust was evaluated on typical bemchmark datasets: CIFAR10, CIFAR-100, STL-10, ImageNet-Dogs, and ImageNet-10. The results presented in Table 3 show that CoHiClust outperforms the comparative methods in 3 out of 5 datasets. It gives extremely good results on CIFAR-10 and ImageNet-10, but is notably worse than CC on STL-10. Nevertheless, one should keep in mind that CoHiClust is the only hierarchical method in this comparison, and constructing a clustering hierarchy, which resembles ground truth classes, is more challenging than directly generating a flat partition.\nTable 3: Comparison with flat clustering methods on datasets of color images.\rWhen analyzing the CIFAR10 and ImageNet-10 datasets, it becomes apparent that CoHiClust generates cluster structures that exhibit a high level of coherence, effectively capturing the underlying patterns within the data. This observation is particularly noteworthy as it aligns closely with our intuitive understanding of how these datasets should be organized. By examining Figure 1 and Figure 7, we can observe how the clusters formed by CoHiClust accurately reflect the semantic relationships present in the images, further reinforcing the validity of the clustering results.\nFig. 7: A tree hierarchy generated by CoHiClust for CIFAR-10. There is an evident distinction into animals (left branch) and machines (right branch). Moreover, all neighbor leaves represent visually similar classes (horses and deers, dogs and cats, trucks and cars, ships and planes). Images with frogs seem to be visually similar to cats and dogs, which leads to their placement in the neighbor leaves (however cats and dogs are connected by a stronger relationship). Interestingly, a small portion of images with horses’ heads are grouped together with birds because of their similar shapes. Although there is a slight mismatch between dogs and cats classes, the left leaf contains pets with bright fur photographed in front, while the right leaf includes animals with dark fur presented from the side, which coincides with our intuition.\nConclusions CoHiClust a contrastive hierarchical clustering suits well to clustering of large-scale image databases. The hierarchical structure constructed by CoHiClust provides significantly more information about the data than typical flat clustering models. In particular, we can inspect the similarity between selected groups by measuring their distance in the hierarchy tree and, in consequence, find super-clusters. Experimental analysis performed on typical clustering benchmarks confirms that the produced partitions are highly similar to ground-truth classes. At the same time, CoHiClust allows us to discover important patterns that have not been encoded in the class labels.\nReferences [1] Liu, Hongfu, et al. “Clustering with outlier removal.” IEEE transactions on knowledge and data engineering 33.6 (2019): 2369-2379.\n[2] Steinbach, Michael, George Karypis, and Vipin Kumar. “A comparison of document clustering techniques.” (2000).\n[3] Oyelade, Jelili, et al. “Clustering algorithms: their application to gene expression data.” Bioinformatics and Biology insights 10 (2016): BBI-S38316.\n[4] Guérin, J., Gibaru, O., Thiery, S., Nyiri, E.: Cnn features are also great at unsupervised classification. arXiv preprint arXiv:1707.01700 (2017).\n[5] Naumov, S., Yaroslavtsev, G., Avdiukhin, D.: Objective-based hierarchical clustering of deep embedding vectors. In: AAAI. pp. 9055–9063 (2021).\n[6] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local structure preservation. In: Ijcai. pp. 1753–1759 (2017).\n[7] Mautz, D., Plant, C., Böhm, C.: Deep embedded cluster tree. In: 2019 IEEE International Conference on Data Mining (ICDM). pp. 1258–1263. IEEE (2019).\n[8] Li, Y., Hu, P., Liu, Z., Peng, D., Zhou, J.T., Peng, X.: Contrastive clustering.In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 8547–8555 (2021).\n[9] Dang, Z., Deng, C., Yang, X., Wei, K., Huang, H.: Nearest neighbor matching for deep clustering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13693–13702 (2021).\n[10] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 119, pp. 1597–1607. PMLR (13–18 Jul 2020), https://proceedings.mlr.press/v119/chen20j.html.\n[11] Frosst, N., Hinton, G.: Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784 (2017).\n[12] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local structure preservation. In: Ijcai. pp. 1753–1759 (2017).\n",
  "wordCount" : "2463",
  "inLanguage": "en",
  "datePublished": "2024-03-24T00:00:00Z",
  "dateModified": "2024-03-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Michał Znaleźniak"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://michalznalezniak.com/posts/post1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Michal Znalezniak",
    "logo": {
      "@type": "ImageObject",
      "url": "https://michalznalezniak.com/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://michalznalezniak.com/" accesskey="h" title="Michal Znalezniak (Alt + H)">Michal Znalezniak</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li></li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://michalznalezniak.com/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://michalznalezniak.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://michalznalezniak.com/">Home</a>&nbsp;»&nbsp;<a href="https://michalznalezniak.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Contrastive Hierarchical Clustering
    </h1>
    <div class="post-meta"><span title='2024-03-24 00:00:00 +0000 UTC'>March 24, 2024</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Michał Znaleźniak

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#clustering" aria-label="Clustering">Clustering</a><ul>
                        
                <li>
                    <a href="#objective-of-clustering" aria-label="Objective of Clustering">Objective of Clustering</a></li>
                <li>
                    <a href="#hierarchical-clustering" aria-label="Hierarchical clustering">Hierarchical clustering</a></li></ul>
                </li>
                <li>
                    <a href="#cohiclust" aria-label="CoHiClust">CoHiClust</a><ul>
                        
                <li>
                    <a href="#backbone-network" aria-label="Backbone network">Backbone network</a></li>
                <li>
                    <a href="#hierarchical-clustering-head" aria-label="Hierarchical clustering head">Hierarchical clustering head</a></li>
                <li>
                    <a href="#loss-functions" aria-label="Loss functions">Loss functions</a></li></ul>
                </li>
                <li>
                    <a href="#results" aria-label="Results">Results</a><ul>
                        
                <li>
                    <a href="#comparison-with-deep-hierarchical-clustering-methods" aria-label="Comparison with deep hierarchical clustering methods">Comparison with deep hierarchical clustering methods</a></li>
                <li>
                    <a href="#comparision-with-agglomerative-hierarchical-clustering" aria-label="Comparision with Agglomerative Hierarchical Clustering">Comparision with Agglomerative Hierarchical Clustering</a></li>
                <li>
                    <a href="#comparision-with-deep-flat-clustering-methods" aria-label="Comparision with deep flat clustering methods">Comparision with deep flat clustering methods</a></li></ul>
                </li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this post, I would like to introduce you to <a href="https://arxiv.org/abs/2303.03389">CoHiClust</a>, a Contrastive Hierarchical
Clustering model based on deep neural networks, which can be applied
to typical image data.  CoHiClust distills the base network
into a binary tree without access to any labeled data.
CoHiClust outperforms the established and popular
Agglomerative Clustering and generates a
hierarchical structure of clusters consistent with
human intuition and image semantics.</p>
<p>You can access the full paper of CoHiClust by clicking <a href="https://arxiv.org/abs/2303.03389">here</a>.</p>
<p>For those who are interested in exploring the code, data, and
additional resources associated with this study, you can find
them in my GitHub repository <a href="https://github.com/MichalZnalezniak/Contrastive-Hierarchical-Clustering">here</a>.</p>
<h1 id="clustering">Clustering<a hidden class="anchor" aria-hidden="true" href="#clustering">#</a></h1>
<p>Clustering, a fundamental branch of unsupervised learning,
is often one of the first steps in data analysis, which finds
applications in anomaly detection [<a href="/posts/post1/#ref-1">1</a>], document clustering [<a href="/posts/post1/#ref-2">2</a>],
bioinformatins [<a href="/posts/post1/#ref-3">3</a>] and many more. Initial approaches use representations
taken from pre-trained models [<a href="/posts/post1/#ref-4">4</a>, <a href="/posts/post1/#ref-5">5</a>] or employ autoencoders in joint
training of the representation and the flat clustering model [<a href="/posts/post1/#ref-6">6</a>] or
hierarchical clustering model [<a href="/posts/post1/#ref-7">7</a>].
Recent models designed to image data frequently follow the
self-supervised learning principle, where the representation
is trained on pairs of similar images generated automatically by
data augmentations [<a href="/posts/post1/#ref-8">8</a>, <a href="/posts/post1/#ref-9">9</a>]. Since augmentations used for image data
are class-invariant, the latter techniques of ten obtain a very
high similarity to the ground truth classes. However,
we should be careful when comparing clustering techniques
only by inspecting their accuracy with ground truth classes
because the objective of clustering is not to perform classification.</p>
<h2 id="objective-of-clustering">Objective of Clustering<a hidden class="anchor" aria-hidden="true" href="#objective-of-clustering">#</a></h2>
<p>So what’s the objective of clustering then? The primary objective of clustering is to discover structures and patterns in high-dimensional unlabeled data and group together data points with similar patterns. The above procedure reduces the complexity, facilitates the interpretation, and grants important insights into data. Let’s examine how much meaningful information a clustering algorithm can deliver when applied to a subset of the most widely recognized datasets in the field of computer vision and machine learning — ImageNet-10.</p>
<p><img loading="lazy" src="images/imagenet-final.png" alt="Fig1"  />

<em>Fig. 1. <strong>Information delivered by hierarchical clustering CoHiClust for ImageNet-10</strong>.
It is evident that images
with soccer ball are similar to pictures with oranges because of their shapes.
Dogs are more similar to leopards than to penguins, which is reflected in the
constructed hierarchy. The same hold when analyzing the leafs representing cars,
trucks and ships. Looking at the first hierarchy level, we observe a distinction on
the right sub-tree representing machines and left-sub-tree dominated by animals.
Moreover, balls and oranges are separated from the animal branch.</em></p>
<p>The algorithm reduces high-dimensional images into a hierarchy of groups that describe images and provides information that summarizes the dataset from high to low-level information.
The hierarchy of groups is consistent with human intuition and image semantics, see Figure 1.</p>
<p>
<center><img src="images/Hierarchy_imagenet.png"></center>
<center>Fig. 2. Super-groups created by clustering algorithm for Image-Net10.</center>
</p>
<p>Thanks to the clustering we can understand that ImageNet-10
consists of two super-groups
which can be categorazied as &lsquo;machines&rsquo; and &rsquo;not machines&rsquo;,
four super-groups which can be catogorazied as &lsquo;flying machines&rsquo;,
&rsquo;not flying machines&rsquo;, &lsquo;animals&rsquo; and &rsquo;not animals&rsquo;, see Figure 2.</p>
<h2 id="hierarchical-clustering">Hierarchical clustering<a hidden class="anchor" aria-hidden="true" href="#hierarchical-clustering">#</a></h2>
<p>Hierarchical clustering organizes data into a tree-like structure where clusters are nested within each other, while flat clustering forms non-overlapping clusters without any hierarchy.</p>
<p>Hierarchical clustering can provide more detailed information about the relationships between data points, as it captures the hierarchical structure of the data. This can be useful for exploring nested relationships and understanding similarities at different levels of granularity.</p>
<p>Hierarchical clustering groups data based on similarity, forming a hierarchical structure of clusters.
Approaches to hierarchical clustering typically belong to two main groups.</p>
<ul>
<li><strong>Agglomerative</strong>: This is a &ldquo;bottom-up&rdquo; approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li>
<li><strong>Divisive</strong>: This is a &ldquo;top-down&rdquo; approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li>
</ul>
<h1 id="cohiclust">CoHiClust<a hidden class="anchor" aria-hidden="true" href="#cohiclust">#</a></h1>
<p>CoHiClust by backpropagation, jointly learns deep representation using
SimCLR [<a href="/posts/post1/#ref-10">10</a>] framework and performs hierarchical clustering in a top-down
manner. There are three key components of CoHiClust, see Figure 3.</p>
<ul>
<li>The <strong>backbone neural network</strong> $f(.)$ that generates the representation used by
the hierarchical clustering head.</li>
<li>The <strong>hierarchical clustering head</strong> $\pi(.)$, which assigns data points to clusters by a
sequence of decisions.</li>
<li>The <strong>regularized contrastive loss</strong>, which allows for training the
whole framework.</li>
</ul>
<p>
<center><img src="images/CoHiClust_Ilustration.png"></center>
<center>Fig. 3: Illustration of main components of CoHiClust.</center>
</p>
<h2 id="backbone-network">Backbone network<a hidden class="anchor" aria-hidden="true" href="#backbone-network">#</a></h2>
<p>CoHiClust utilizes ResNet architectures as its backbone network.
This component projects images into an internal representation,
which is then utilized by the clustering network.
CoHiClust can rely on various architecture options.
You can observe how change of backbone network influences the final quality
of clustering in the <a href="https://arxiv.org/abs/2303.03389">full paper</a>.</p>
<h2 id="hierarchical-clustering-head">Hierarchical clustering head<a hidden class="anchor" aria-hidden="true" href="#hierarchical-clustering-head">#</a></h2>
<p>CoHiClust depends on a soft binary decision tree to create a hierarchical structure, where leaves
play the role of clusters (similar to [<a href="/posts/post1/#ref-11">11</a>]
). In contrast to hard decision trees, every
internal node defines the probability of taking a left/right branch. The final
assignment of the input examples to clusters involves partial decisions made by
the internal nodes. Aggregating these decisions induces the posterior probability
over leaves</p>
<p>Each inner node is model by one neuron from
linear layer $\pi(z) = [\sigma (w_1^T z + b_1), \ldots, \sigma (w_K^T z + b_K)]$,
where $w_n \in \R^N$ and $b_n \in \R$ are
trainable parameters of $\pi$ and $\sigma$ is sigmoid function.</p>
<p><img loading="lazy" src="images/DIAGRAM_COMBINED_2.png" alt="Fig4"  />

<em>Fig. 4: <strong>Illustration of distilling neurnal network into soft decision tree</strong>.
The output neurons  of the projection head $\pi$ (appended to the base
network $g$) model decisions made by the internal
tree nodes. The final assignment of the input example
to the cluster (leaf node) is performed by aggregating
edge probabilities located on the path from the root
to this leaf</em></p>
<p>With  $\pi$ output we can define a probability distribution
of assigning data to clusters on all levels of the tree, see Figure 4</p>
<h2 id="loss-functions">Loss functions<a hidden class="anchor" aria-hidden="true" href="#loss-functions">#</a></h2>
<p>CoHiClust is trained with hierarchical contrastive loss function
designed for trees - CoHiLoss. CoHiClust builds hierarchical structure by
maximizing the likelihood that similar data points will
follow the same path. The more similar data points, the longer
they should be routed through the same nodes.
Clustering works in unsupervised setting, therefor CoHiClust
uses a self-supervised approach and generates images using data
augmentations (SimCLR).</p>
<p>Consider two data points $x_1, x_2$ with posterior probabilities
$P_t(x_1), P_t(x_2)$ at level $t$.
The probability they reach the same node is
$P_t(x_1) \cdot P_t(x_2) = \sum_{i=0}^{2^t-1} P_t^i(x_1) P_t^i(x_2)$,
maximal if both are identical one-hot vectors.
Let&rsquo;s Define the similarity score
$s_t(x_1,x_2) = \sqrt{P_t(x_1) \cdot P_t(x_2)} = \sum_{i=0}^{2^t-1} \sqrt{P_t^i(x_1) P_t^i(x_2)}$.
Let&rsquo;s aggregate it over all levels of the tree,
the final similarity function
is $s(x_1,x_2) = \sum_{t=0}^{T-1} s_t(x_1,x_2)$.</p>
<p>In training, consider a minibatch $ [{x_j}]_{j=1}^N $ and its augmented view $ \tilde{x_j} $. Each $(x_j,\tilde{x}_j)$ pair is treated as positive, maximizing their similarity score to encourage them to share the same leaf node. Other pairs are treated as negative to avoid degenerate solutions, yielding the hierarchical contrastive loss:</p>
<p>$$ CoHiLossNeg = \frac{1}{N(N-1)} \sum_{j=1}^N \sum_{i\neq j} s(x_j,\tilde{x}_{i}) $$</p>
<p>$$ CoHiLossPos = \frac{1}{N} \sum_{j=1}^N s(x_j,\tilde{x}_{j}) $$</p>
<p>$$ CoHiLoss = -CoHiLossNeg + CoHiLossPos $$</p>
<p>Minimizing this loss maximizes the likelihood of similar data points sharing the same path (second term) and minimizes the likelihood of dissimilar ones being grouped together.</p>
<h1 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h1>
<p>CoHiClust was evaluated on several datasets of color images of
various resolutions and with a diverse number of classes.
In addition to reporting similarity scores with ground-truth
partitions, the constructed hierarchies were analyzed,
which is equally important in practical use-cases.</p>
<p>To measure the similarity of the constructed partition with
the ground truth, four widely-used clustering metrics were
applied: normalized mutual information (NMI), clustering accuracy
(ACC), adjusted rand index (ARI), and dendrogram purity (DP).</p>
<p><img loading="lazy" src="images/tree_mnist.png" alt="Fig5"  />

<em>Fig. 5: <strong>Tree hierarchy constructed for MNIST</strong>. Observe that neighboring
leaves contain images of visually similar classes, e.g. 8 and 0; 4 and 9; 1 and 7.
Such a property holds also for nodes in the upper levels of the tree – the left
sub-tree contain digits with circular shapes, while the digits located in the right
sub-tree consist of lines.</em></p>
<h2 id="comparison-with-deep-hierarchical-clustering-methods">Comparison with deep hierarchical clustering methods<a hidden class="anchor" aria-hidden="true" href="#comparison-with-deep-hierarchical-clustering-methods">#</a></h2>
<p>In authors knowledge, DeepECT [<a href="/posts/post1/#ref-7">7</a>] is the only hierarchical clustering method based
on deep neural networks. Following their experimental setup, we report the results
on two popular image datasets, MNIST and F-MNIST, and consider classical
hierarchical algorithms evaluated on the latent representation created by the
autoencoder and IDEC [<a href="/posts/post1/#ref-12">12</a>].
The results summarized in Table 1 demonstrate that CoHiClust outperforms
all baselines on both MNIST and F-MNIST datasets in terms of all metrics.
Interestingly, DeepECT benefits from data augmentation in the case of MNIST,
while on F-MNIST it deteriorates its performance. All methods except CoHiClust
and DeepECT failed completely to create a hierarchy recovering true classes (see
the DP measure), which confirms that there is a lack of powerful hierarchical
clustering methods based on neural networks.</p>
<p style="text-align: center">
<em>Table 1: Comparison with hierarchical models in terms of DP, NMI and ACC (higher is better).</em>
<img src="images/CoHiClust_vs_DeepECT.png">
</p>
<p>While examining the MNIST and F-MNIST datasets,
it&rsquo;s apparent that CoHiClust produces clusters with a
sensible structure. This structure is in line with our
expectations and the inherent semantics of the images.
Figure 5 and Figure 6 provide visual confirmation of this alignment.</p>
<p><img loading="lazy" src="images/tree_fmnist.png" alt="Fig6"  />

<em>Fig. 6: <strong>Tree hierarchy generated by CoHiClust for F-MNIST</strong> (images
in the nodes denote mean images in each sub-tree). The right sub-tree contains
clothes while the other items (shoes and bags) are placed in the left branch.
Looking at the lowest hierarchy level, we have clothes with long sleeves grouped
in the neighboring leaves. The same holds for clothes with designs. Observe that
CoHiClust assigned white-colored t-shirts and dresses to the same cluster, while
trousers are in the separate one. Small shoes such as sneakers or sandals are
considered similar (neighboring leaves) and distinct from ankle shoes. Concluding,
CoHiClust is able to retrieve meaningful information about image semantics,
which is complementary to the ground truth classification.</em></p>
<h2 id="comparision-with-agglomerative-hierarchical-clustering">Comparision with Agglomerative Hierarchical Clustering<a hidden class="anchor" aria-hidden="true" href="#comparision-with-agglomerative-hierarchical-clustering">#</a></h2>
<p>The top layer responsible for constructing a decision tree is an
important component of CoHiClust and cannot be replaced by
alternative hierarchical clustering methods.
For this purpose, a backbone network is first trained with a
typical self-supervised SimCLR technique. Next, agglomerative
clustering is applied to the resulting representation.
As seen in Table 2, agglomerative clustering yields very low
results, indicating that joint optimization of the backbone
network and clustering tree using the proposed CoHiLoss
is a significantly better choice. Consequently,
the representation taken from a typical self-supervised
learning model does not provide a representation that can
be clustered accurately using simple methods.</p>
<p>
<center>Table 2: Comparison with agglomerative clustering trained on the representation
generated by the self-supervised learning model on CIFAR10.</center>
<center><img src="images/CoHiCLust_vs_AC.png"></center>
</p>
<h2 id="comparision-with-deep-flat-clustering-methods">Comparision with deep flat clustering methods<a hidden class="anchor" aria-hidden="true" href="#comparision-with-deep-flat-clustering-methods">#</a></h2>
<p>CoHiClust was evaluated on typical bemchmark datasets:
CIFAR10, CIFAR-100, STL-10, ImageNet-Dogs, and ImageNet-10.
The results presented in Table 3 show that CoHiClust outperforms the
comparative methods in 3 out of 5 datasets. It gives extremely good results
on CIFAR-10 and ImageNet-10, but is notably worse than CC on STL-10.
Nevertheless,
one should keep in mind that CoHiClust is the only hierarchical method in this
comparison, and constructing a clustering hierarchy, which resembles ground
truth classes, is more challenging than directly generating a flat partition.</p>
<p style="text-align: center">
<em>Table 3: Comparison with flat clustering methods on datasets of color images.</em>
<img src="images/CoHiClust_vs_flat.png">
</p>
<p>When analyzing the CIFAR10 and ImageNet-10 datasets, it becomes
apparent that CoHiClust generates cluster structures that
exhibit a high level of coherence, effectively capturing the
underlying patterns within the data. This observation is
particularly noteworthy as it aligns closely with our
intuitive understanding of how these datasets should be
organized. By examining Figure 1 and Figure 7, we can observe
how the clusters formed by CoHiClust accurately reflect the
semantic relationships present in the images, further
reinforcing the validity of the clustering results.</p>
<p><img loading="lazy" src="images/cifar10_final.png" alt="Fig7"  />

<em>Fig. 7: <strong>A tree hierarchy generated by CoHiClust for CIFAR-10</strong>. There is
an evident distinction into animals (left branch) and machines (right branch).
Moreover, all neighbor leaves represent visually similar classes (horses and deers,
dogs and cats, trucks and cars, ships and planes). Images with frogs seem to be
visually similar to cats and dogs, which leads to their placement in the neighbor
leaves (however cats and dogs are connected by a stronger relationship). Interestingly, a small portion of images with horses’ heads are grouped together with
birds because of their similar shapes. Although there is a slight mismatch between
dogs and cats classes, the left leaf contains pets with bright fur photographed in
front, while the right leaf includes animals with dark fur presented from the side,
which coincides with our intuition.</em></p>
<h1 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h1>
<p>CoHiClust a contrastive hierarchical clustering suits
well to clustering of large-scale image databases. The hierarchical structure
constructed by CoHiClust provides significantly more information about the data
than typical flat clustering models. In particular, we can inspect the similarity
between selected groups by measuring their distance in the hierarchy tree and,
in consequence, find super-clusters. Experimental analysis performed on typical
clustering benchmarks confirms that the produced partitions are highly similar
to ground-truth classes. At the same time, CoHiClust allows us to discover
important patterns that have not been encoded in the class labels.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p><a id="ref-1" color="#FFFFFF">[1] Liu, Hongfu, et al. &ldquo;Clustering with outlier removal.&rdquo; IEEE transactions on knowledge and data engineering 33.6 (2019): 2369-2379.</a></p>
<p><a id="ref-2">[2] Steinbach, Michael, George Karypis, and Vipin Kumar. &ldquo;A comparison of document clustering techniques.&rdquo; (2000).</a></p>
<p><a id="ref-3">[3] Oyelade, Jelili, et al. &ldquo;Clustering algorithms: their application to gene expression data.&rdquo; Bioinformatics and Biology insights 10 (2016): BBI-S38316.</a></p>
<p><a id="ref-4">[4] Guérin, J., Gibaru, O., Thiery, S., Nyiri, E.: Cnn features are also great at unsupervised classification. arXiv preprint arXiv:1707.01700 (2017).</a></p>
<p><a id="ref-5">[5] Naumov, S., Yaroslavtsev, G., Avdiukhin, D.: Objective-based hierarchical clustering of deep embedding vectors. In: AAAI. pp. 9055–9063 (2021).</a></p>
<p><a id="ref-6">[6] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local structure preservation. In: Ijcai. pp. 1753–1759 (2017).</a></p>
<p><a id="ref-7">[7] Mautz, D., Plant, C., Böhm, C.: Deep embedded cluster tree. In: 2019 IEEE International Conference on Data Mining (ICDM). pp. 1258–1263. IEEE (2019).</a></p>
<p><a id="ref-8">[8] Li, Y., Hu, P., Liu, Z., Peng, D., Zhou, J.T., Peng, X.: Contrastive clustering.In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 8547–8555 (2021).</a></p>
<p><a id="ref-9">[9] Dang, Z., Deng, C., Yang, X., Wei, K., Huang, H.: Nearest neighbor matching for deep clustering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13693–13702 (2021).</a></p>
<p><a id="ref-10">[10] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 119, pp. 1597–1607. PMLR (13–18 Jul 2020), <a href="https://proceedings.mlr.press/v119/chen20j.html">https://proceedings.mlr.press/v119/chen20j.html</a>.</a></p>
<p><a id="ref-11">[11] Frosst, N., Hinton, G.: Distilling a neural network into a soft decision tree. arXiv
preprint arXiv:1711.09784 (2017).</a></p>
<p><a id="ref-12">[12] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local
structure preservation. In: Ijcai. pp. 1753–1759 (2017).</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://michalznalezniak.com/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://michalznalezniak.com/tags/clustering/">Clustering</a></li>
      <li><a href="https://michalznalezniak.com/tags/representation-learning/">Representation Learning</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on x"
            href="https://x.com/intent/tweet/?text=Contrastive%20Hierarchical%20Clustering&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f&amp;hashtags=deeplearning%2cclustering%2crepresentationlearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f&amp;title=Contrastive%20Hierarchical%20Clustering&amp;summary=Contrastive%20Hierarchical%20Clustering&amp;source=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f&title=Contrastive%20Hierarchical%20Clustering">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on whatsapp"
            href="https://api.whatsapp.com/send?text=Contrastive%20Hierarchical%20Clustering%20-%20https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on telegram"
            href="https://telegram.me/share/url?text=Contrastive%20Hierarchical%20Clustering&amp;url=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Contrastive Hierarchical Clustering on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Contrastive%20Hierarchical%20Clustering&u=https%3a%2f%2fmichalznalezniak.com%2fposts%2fpost1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://michalznalezniak.com/">Michal Znalezniak</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
